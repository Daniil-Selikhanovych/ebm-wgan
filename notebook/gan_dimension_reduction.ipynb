{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch, torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import autograd\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "from utils import prepare_train_batches, epoch_visualization\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "device_default = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def calc_gradient_penalty(D, real_data, fake_data, batch_size, Lambda = 0.1,\n",
    "                          device = device_default):\n",
    "    #print(real_data.shape)\n",
    "    #print(fake_data.shape)\n",
    "    alpha = torch.rand(batch_size, 1)\n",
    "    alpha = alpha.expand(real_data.size()).to(device)\n",
    "\n",
    "    interpolates = alpha * real_data + (1 - alpha) * fake_data\n",
    "    interpolates = interpolates.to(device)\n",
    "    interpolates = autograd.Variable(interpolates, \n",
    "                                     requires_grad = True)\n",
    "    discriminator_interpolates = D(interpolates)\n",
    "    ones = torch.ones(discriminator_interpolates.size()).to(device)\n",
    "    gradients = autograd.grad(outputs = discriminator_interpolates, \n",
    "                              inputs = interpolates,\n",
    "                              grad_outputs = ones,\n",
    "                              create_graph = True, \n",
    "                              retain_graph = True, \n",
    "                              only_inputs = True)[0]\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1.0) ** 2).mean() * Lambda\n",
    "    return gradient_penalty\n",
    "    \n",
    "def train_wgan(X_train,\n",
    "               X_train_batches, \n",
    "               generator, g_optimizer, \n",
    "               discriminator, d_optimizer,\n",
    "               path_to_save,\n",
    "               batch_size = 256,\n",
    "               device = device_default,\n",
    "               use_gradient_penalty = True,\n",
    "               Lambda = 0.1,\n",
    "               num_epochs = 20000, \n",
    "               num_epoch_for_save = 100,\n",
    "               batch_size_sample = 5000,\n",
    "               proj_list = None):\n",
    "\n",
    "    k_g = 1\n",
    "    generator_loss_arr = []\n",
    "    generator_mean_loss_arr = []\n",
    "    discriminator_loss_arr = []\n",
    "    discriminator_mean_loss_arr = []\n",
    "    one = torch.tensor(1, dtype = torch.float).to(device)\n",
    "    mone = one * -1\n",
    "    mone = mone.to(device)  \n",
    "    path_to_save_models = os.path.join(path_to_save, 'models')\n",
    "    path_to_save_plots = os.path.join(path_to_save, 'plots')\n",
    "\n",
    "    try:\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Start epoch = {epoch}\")\n",
    "            if epoch < 25:\n",
    "                k_d = 100\n",
    "            else: \n",
    "                k_d = 10\n",
    "\n",
    "            for p in discriminator.parameters():  # reset requires_grad\n",
    "                p.requires_grad = True\n",
    "            \n",
    "            start_time = time.time()\n",
    "            # Optimize D\n",
    "            # discriminator.train(True)\n",
    "            # generator.train(False)\n",
    "            for _ in range(k_d):\n",
    "                # Sample noise\n",
    "                # real_data = sample_data_batch(batch_size, \n",
    "                #                               X_train, \n",
    "                #                               device)\n",
    "                discriminator.zero_grad()\n",
    "                real_data = next(X_train_batches)\n",
    "                if (real_data.shape[0] != batch_size):\n",
    "                    continue\n",
    "\n",
    "                real_data = torch.Tensor(real_data)\n",
    "                real_data = autograd.Variable(real_data).to(device)\n",
    "                \n",
    "                d_real_data = discriminator(real_data).mean()\n",
    "                d_real_data.backward(mone)   \n",
    "                \n",
    "                noise = generator.make_hidden(batch_size)\n",
    "                #with torch.no_grad():\n",
    "                noise = autograd.Variable(noise).to(device)\n",
    "                #print(noise.size())\n",
    "                fake_data = generator(noise)\n",
    "                d_fake_data = discriminator(fake_data).mean()\n",
    "                d_fake_data.backward(one)\n",
    "\n",
    "                d_loss = d_fake_data - d_real_data\n",
    "                #print(\"OK\")\n",
    "                if use_gradient_penalty:\n",
    "                    gradient_penalty = calc_gradient_penalty(discriminator, \n",
    "                                                             real_data.data, \n",
    "                                                             fake_data.data, \n",
    "                                                             batch_size,\n",
    "                                                             Lambda)\n",
    "                    gradient_penalty.backward()\n",
    "                    d_loss += gradient_penalty\n",
    "                d_optimizer.step()\n",
    "                discriminator_loss_arr.append(d_loss.data.cpu().numpy())\n",
    "\n",
    "            #discriminator.train(False)\n",
    "            #generator.train(True)\n",
    "            # Optimize G\n",
    "            for p in discriminator.parameters():  # to avoid computation\n",
    "                p.requires_grad = False\n",
    "\n",
    "            for _ in range(k_g):\n",
    "                g_optimizer.zero_grad()\n",
    "\n",
    "                # Do an update\n",
    "                noise = generator.make_hidden(batch_size)\n",
    "                noise = autograd.Variable(noise).to(device)\n",
    "                fake_data = generator(noise)\n",
    "\n",
    "                generator_loss = discriminator(fake_data).mean()\n",
    "                generator_loss.backward(mone)\n",
    "                generator_loss = -generator_loss\n",
    "                g_optimizer.step()\n",
    "                generator_loss_arr.append(generator_loss.data.cpu().numpy())\n",
    "           \n",
    "            end_time = time.time()\n",
    "            calc_time = end_time - start_time\n",
    "            discriminator_mean_loss_arr.append(np.mean(discriminator_loss_arr[-k_d :]))\n",
    "            generator_mean_loss_arr.append(np.mean(generator_loss_arr[-k_g :]))\n",
    "            print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "                   epoch + 1, num_epochs, calc_time))\n",
    "            print(\"Discriminator last mean loss: \\t{:.6f}\".format(\n",
    "                   discriminator_mean_loss_arr[-1]))\n",
    "            print(\"Generator last mean loss: \\t{:.6f}\".format(\n",
    "                   generator_mean_loss_arr[-1])) \n",
    "            if epoch % num_epoch_for_save == 0:\n",
    "                # Visualize\n",
    "                epoch_visualization(X_train, generator, \n",
    "                                   use_gradient_penalty, \n",
    "                                   discriminator_mean_loss_arr, \n",
    "                                   epoch, Lambda,\n",
    "                                   generator_mean_loss_arr, \n",
    "                                   path_to_save_plots,\n",
    "                                   batch_size_sample,\n",
    "                                   proj_list)\n",
    "               \n",
    "                cur_time = datetime.datetime.now().strftime('%Y_%m_%d-%H_%M_%S')\n",
    "                discriminator_model_name = cur_time + '_discriminator.pth'\n",
    "                generator_model_name = cur_time + '_generator.pth'\n",
    "\n",
    "                path_to_discriminator = os.path.join(path_to_save_models, discriminator_model_name)\n",
    "                path_to_generator = os.path.join(path_to_save_models, generator_model_name)\n",
    "\n",
    "                torch.save(discriminator.state_dict(), path_to_discriminator)\n",
    "                torch.save(generator.state_dict(), path_to_generator)\n",
    "                \n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
